var documenterSearchIndex = {"docs":
[{"location":"#PowerSimulationNODE-Documentation","page":"PowerSimulationNODE Documentation","title":"PowerSimulationNODE Documentation","text":"","category":"section"},{"location":"","page":"PowerSimulationNODE Documentation","title":"PowerSimulationNODE Documentation","text":"NODETrainParams","category":"page"},{"location":"#PowerSimulationNODE.NODETrainParams","page":"PowerSimulationNODE Documentation","title":"PowerSimulationNODE.NODETrainParams","text":"mutable struct NODETrainParams\n\nFields\n\ntrain_id::Int64: id for the training instance, used for naming output data folder.\nsolver::String: solver used for the NODE problem. Valid Values [\"Rodas4\"]\nsolver_tols:: Tuple{Float64, Float64}: solver tolerances (abstol, reltol).\nsensealg::String: sensitivity algorithm used in training. Valid Values [\"ForwardDiff\", \"Zygote\" ]\noptimizer::[\"Adam\", \"Bfgs\"]: main optimizer used in training.\noptimizer_η::Float64: Learning rate for Adam (amount by which gradients are discounted before updating weights). Ignored if Adam is not the optimizer.\n`optimizer_adjust::String: optimizer used for final adjustments (2nd stage). Valid values [\"Adam\", \"Bfgs\", \"nothing\"].\noptimizer_adjust_η::Float64: Learning rate for Adam (amount by which gradients are discounted before updating weights). Ignored if Adam is not the optimizer.\nmaxiters::Int64: The maximum possible iterations for the entire training instance. If lb_loss = 0 and optimizer = \"Adam\" the training should never exit early and maxiters will be hit.   Note that the number of saved data points can exceed maxiters because there is an additional callback at the end of each individual optimization.\nlb_loss::Float64: If the value of the loss function moves below lb_loss during training, the current optimization ends (current range).\ntraining_groups::DataStructures.SortedDict{   Tuple{Float64, Float64},   NamedTuple{       (:shoot_times, :multiple_shoot_continuity_term, :batching_sample_factor),       Tuple{Int64, Float64, Float64},   }: Specify the tspan for each group of training, and the multiple shooting and random batching parameter for each group.  \ngroupsize_faults::Int64: Number of faults trained on simultaneous 1:sequential training. if equal to number of pvs in sys_train, parallel training.\nloss_function_weights::Tuple{Float64, Float64}: weights used for loss function (mae_weight, mse_weight).\nloss_function_scale::String: Scaling of the loss function.  \"range\": the range of the real current and imaginary current are used to scale both the mae. Valid values [\"range\", \"none\"]   and mse portions of the loss function. The goal is to give equal weight to real and imaginary components even if the magnitude of the disturbance differs. \"none\": no additional scaling applied.\node_model::String [\"none\",\"vsm\"]: The ode model used in conjunction with the NODE during training. \"none\" uses a purely data driven NODE surrogate. \nnode_input_scale::Float64: Scale factor on the voltage input to the NODE. Does not apply to other inputs (ie the feedback states).\nnode_output_scale::Float64: Scale factor on the current output of the NODE. Does not apply to other outputs (ie the feedback states).\nnode_state_inputs::Vector{Tuple{String, Symbol: Additional states that are input to NODE: (\"DeviceName\", :StateSymbol). The device name and symbol must match the solution objected passed as input data. \nnode_unobserved_states::Int64: Number of feedback states in the NODE. Does not include the output current states which can be feedback if node_feedback_current = true.\nnode_feedback_current::Bool: Determines if current is also a feedback state.\nnode_layers::Int64: Number of hidden layers in the NODE. Does not include the input or output layer.\nnode_width::Int64: Number of neurons in each hidden layer. Each hidden layer has the same number of neurons. The width of the input and output layers are determined by the combination of other parameters.\nnode_activation::String: Activation function for NODE. The output layer always uses the identity activation. Valid Values [\"relu\", \"hardtanh\", \"sigmoid\"]\nrng_seed::Int64: Seed for the random number generator used for initializing the NN for reproducibility across training runs.\noutput_mode::Int: 1: do not collect any data during training, only save high-level data related to training and final results 2: Same as 1, also save value of loss throughout training. Valid values [1,2,3]   3: same as 2, also save parameters and predictions during training.\nbase_path:String: Directory for training where input data is found and output data is written.\ninput_data_path:String: From base_path, the directory for input data.\noutput_data_path:String: From base_path, the directory for saving output data.\nverify_psid_node_off:Bool: true: before training, check that the surrogate with NODE turned off matches the data provided from PSID simulation.\ngraphical_report_mode:Int64: 0: do not generate plots. 1: plot final result only. 2 plot for transitions between faults. 3: plot for transitions between ranges. 4: plot for every train iteration.\n\n\n\n\n\n","category":"type"},{"location":"","page":"PowerSimulationNODE Documentation","title":"PowerSimulationNODE Documentation","text":"```","category":"page"}]
}
